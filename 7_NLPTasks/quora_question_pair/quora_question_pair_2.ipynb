{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np, torch, re\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "from torch.nn import Embedding\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ques1': [5, 10], 'ques2': [2, 3, 4, 5, 6, 5, 7, 8, 9, 10, 11, 12, 14], 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/srishtysuman/PycharmProjects/NaturalLanguageProcessing/quora_question_pair_data/train.csv')\n",
    "df=df.iloc[:2000]\n",
    "df\n",
    "def clean(text):\n",
    "    if text=='' or text is pd.isnull(text) or type(text)!=str:\n",
    "        return ''\n",
    "    text=text.lower()\n",
    "    text=re.sub(r\"([.!?])\", r\" \\1\", text)\n",
    "    text=re.sub(r\"[^a-zA-Z]+\", r\" \", text)\n",
    "\n",
    "    return text\n",
    "df=df[[\"question1\", \"question2\", \"is_duplicate\"]]\n",
    "df[\"question1\"]=df[\"question1\"].apply(lambda x: clean(x))\n",
    "df[\"question2\"]=df[\"question2\"].apply(lambda x: clean(x))\n",
    "df\n",
    "\n",
    "class VocabularyNMapSentence:\n",
    "    def __init__(self):\n",
    "        self.word2index={}\n",
    "        self.index2word={'SOS':1, 'EOS':2}\n",
    "        self.n_word=2\n",
    "        self.word2count={}\n",
    "    def addSentence(self, q):\n",
    "        q_map=[]\n",
    "        for word in q.split(' '):\n",
    "            if word in self.word2index:\n",
    "                q_map.append(self.word2index[word])\n",
    "            else:\n",
    "                self.word2index[word]=self.n_word\n",
    "                self.index2word[self.n_word]=word\n",
    "                self.n_word+=1\n",
    "                self.word2count[word]=0\n",
    "            self.word2count[word]+=1\n",
    "        return q_map\n",
    "    \n",
    "build_vocab_and_map=VocabularyNMapSentence()\n",
    "df[\"q1_map\"]=df[\"question1\"].apply(lambda x: build_vocab_and_map.addSentence(x))\n",
    "df[\"q2_map\"]=df[\"question2\"].apply(lambda x: build_vocab_and_map.addSentence(x))\n",
    "df = df[df['q1_map'].apply(lambda x: len(x) > 0)].reset_index(drop=True)\n",
    "df = df[df['q2_map'].apply(lambda x: len(x) > 0)].reset_index(drop=True)\n",
    "df\n",
    "ques1=df[\"q1_map\"].tolist()\n",
    "ques2=df[\"q2_map\"].tolist()\n",
    "labels=df[\"is_duplicate\"].tolist()\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, ques1, ques2, label):\n",
    "        self.ques1=ques1\n",
    "        self.ques2=ques2\n",
    "        self.labels=label\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"ques1\": self.ques1[index],\n",
    "            \"ques2\": self.ques2[index],\n",
    "            \"label\": self.labels[index]\n",
    "        }\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    ques1_tensor=[]\n",
    "    ques2_tensor=[]\n",
    "    label_tensor=[]\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        ques1_tensor.append(batch[i][\"ques1\"])\n",
    "        ques2_tensor.append(batch[i][\"ques2\"])\n",
    "        label_tensor.append(batch[i][\"label\"])\n",
    "    q1_length=[len(q) for q in ques1_tensor]\n",
    "    q2_length=[len(q) for q in ques2_tensor]\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"q1\":ques1_tensor,\n",
    "        \"q2\":ques2_tensor,\n",
    "        \"q1_length\":q1_length,\n",
    "        \"q2_length\":q2_length,\n",
    "        \"label\":label_tensor\n",
    "    }\n",
    "\n",
    "train_data=Dataset(ques1, ques2, labels)\n",
    "for data in train_data:\n",
    "    print(data)\n",
    "    break\n",
    "train_dataloader=DataLoader(train_data, batch_size=32, collate_fn=collate_fn)\n",
    "for sample in train_dataloader:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([0.0026, 0.0421, 0.0335, 0.0177, 0.0627, 0.0347, 0.0357, 0.0535, 0.0502,\n",
      "        0.0241, 0.0256, 0.0446, 0.0760, 0.0257, 0.1102, 0.0361, 0.0430, 0.0459,\n",
      "        0.0303, 0.0401, 0.0451, 0.0290, 0.1626, 0.0385, 0.0763, 0.0311, 0.0557,\n",
      "        0.0478, 0.0210, 0.0291, 0.0623, 0.0397], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.])\n",
      "tensor([0.0703, 0.0559, 0.0674, 0.0914, 0.0940, 0.0374, 0.0662, 0.0574, 0.1340,\n",
      "        0.0657, 0.1076, 0.0873, 0.0890, 0.0638, 0.0561, 0.0943, 0.0621, 0.2254,\n",
      "        0.0611, 0.1287, 0.1958, 0.1402, 0.0384, 0.0344, 0.0820, 0.0942, 0.0320,\n",
      "        0.0584, 0.0545, 0.0777, 0.0786, 0.0675], grad_fn=<CopySlices>) tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.])\n",
      "tensor([0.0797, 0.1066, 0.1509, 0.1263, 0.1308, 0.1240, 0.1273, 0.0954, 0.0934,\n",
      "        0.0877, 0.0685, 0.0835, 0.0777, 0.0974, 0.1254, 0.1734, 0.0711, 0.1382,\n",
      "        0.0975, 0.1120, 0.1450, 0.2363, 0.1521, 0.1768, 0.0783, 0.0910, 0.1877,\n",
      "        0.1823, 0.0610, 0.1185, 0.1085, 0.1040], grad_fn=<CopySlices>) tensor([0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.])\n",
      "tensor([0.1302, 0.1481, 0.2091, 0.0907, 0.1083, 0.0773, 0.1995, 0.1801, 0.0925,\n",
      "        0.1765, 0.1284, 0.1449, 0.2798, 0.0029, 0.1257, 0.1207, 0.1561, 0.1598,\n",
      "        0.1311, 0.2094, 0.0939, 0.1373, 0.2130, 0.1598, 0.2148, 0.1413, 0.1491,\n",
      "        0.0973, 0.1715, 0.1169, 0.2048, 0.1702], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1.])\n",
      "tensor([0.2089, 0.3030, 0.2708, 0.1412, 0.2632, 0.1838, 0.2380, 0.2178, 0.2116,\n",
      "        0.1564, 0.0828, 0.3044, 0.2411, 0.1758, 0.1631, 0.1200, 0.1371, 0.2236,\n",
      "        0.2503, 0.1144, 0.1406, 0.1514, 0.1851, 0.2334, 0.1027, 0.1925, 0.2265,\n",
      "        0.1972, 0.0141, 0.4234, 0.1905, 0.3501], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
      "tensor([0.1533, 0.2040, 0.2523, 0.1146, 0.2446, 0.3667, 0.2986, 0.2828, 0.2160,\n",
      "        0.2958, 0.1721, 0.3362, 0.2751, 0.1815, 0.2815, 0.1946, 0.1962, 0.2149,\n",
      "        0.2296, 0.3258, 0.3651, 0.1722, 0.2542, 0.1687, 0.1736, 0.3853, 0.2359,\n",
      "        0.1703, 0.2603, 0.3383, 0.3021, 0.3318], grad_fn=<CopySlices>) tensor([1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.])\n",
      "tensor([0.3103, 0.2368, 0.3143, 0.2812, 0.3342, 0.2400, 0.3004, 0.2940, 0.2461,\n",
      "        0.2486, 0.3184, 0.2648, 0.2041, 0.2250, 0.2912, 0.3393, 0.3120, 0.3239,\n",
      "        0.3663, 0.2374, 0.1773, 0.2777, 0.2951, 0.2876, 0.2244, 0.3037, 0.2782,\n",
      "        0.3282, 0.3211, 0.2522, 0.3805, 0.3250], grad_fn=<CopySlices>) tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.])\n",
      "tensor([0.3198, 0.2382, 0.4577, 0.3445, 0.3124, 0.2599, 0.4241, 0.3171, 0.2929,\n",
      "        0.2733, 0.3086, 0.3003, 0.2622, 0.3654, 0.3222, 0.4163, 0.3655, 0.2344,\n",
      "        0.3597, 0.2597, 0.3395, 0.3650, 0.3979, 0.3314, 0.3732, 0.3641, 0.3195,\n",
      "        0.2791, 0.2523, 0.2348, 0.3271, 0.4574], grad_fn=<CopySlices>) tensor([1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.])\n",
      "tensor([0.3918, 0.3279, 0.2468, 0.5317, 0.3288, 0.4369, 0.4312, 0.2736, 0.3520,\n",
      "        0.3152, 0.2585, 0.3199, 0.4577, 0.3849, 0.2929, 0.3738, 0.2356, 0.4900,\n",
      "        0.5386, 0.2444, 0.3682, 0.6101, 0.3421, 0.2748, 0.3032, 0.4376, 0.4125,\n",
      "        0.2756, 0.3170, 0.4068, 0.3402, 0.4064], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.])\n",
      "tensor([0.4685, 0.4943, 0.5853, 0.4395, 0.4535, 0.5338, 0.3611, 0.3066, 0.4873,\n",
      "        0.3524, 0.3671, 0.4080, 0.4392, 0.4785, 0.4263, 0.3976, 0.3870, 0.5235,\n",
      "        0.3541, 0.3908, 0.4798, 0.4471, 0.3996, 0.4535, 0.4660, 0.4707, 0.3772,\n",
      "        0.5625, 0.4045, 0.5110, 0.3140, 0.4156], grad_fn=<CopySlices>) tensor([1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.])\n",
      "tensor([0.4242, 0.4065, 0.4839, 0.4713, 0.4211, 0.4128, 0.3525, 0.4014, 0.3169,\n",
      "        0.3547, 0.4547, 0.4830, 0.4644, 0.4405, 0.3570, 0.3788, 0.2566, 0.4918,\n",
      "        0.5019, 0.3828, 0.4552, 0.5584, 0.4212, 0.3654, 0.5120, 0.3810, 0.4036,\n",
      "        0.3682, 0.4275, 0.4630, 0.3754, 0.4718], grad_fn=<CopySlices>) tensor([0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.])\n",
      "tensor([0.4500, 0.4022, 0.4677, 0.0096, 0.4896, 0.3499, 0.4721, 0.4196, 0.5338,\n",
      "        0.4585, 0.5691, 0.5293, 0.5178, 0.4237, 0.3639, 0.4208, 0.3572, 0.3487,\n",
      "        0.4865, 0.4278, 0.5170, 0.3824, 0.4024, 0.5231, 0.3756, 0.5205, 0.5003,\n",
      "        0.5210, 0.4108, 0.4985, 0.5279, 0.3856], grad_fn=<CopySlices>) tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.])\n",
      "tensor([0.5402, 0.5220, 0.3573, 0.2688, 0.5923, 0.4059, 0.5086, 0.4296, 0.3889,\n",
      "        0.3222, 0.5262, 0.4106, 0.4303, 0.4132, 0.4096, 0.4718, 0.4005, 0.5247,\n",
      "        0.5905, 0.4388, 0.5074, 0.4624, 0.3783, 0.3723, 0.4983, 0.4415, 0.4069,\n",
      "        0.4454, 0.4571, 0.3726, 0.3805, 0.4112], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
      "tensor([0.3196, 0.4528, 0.3758, 0.4161, 0.4076, 0.5050, 0.5463, 0.4949, 0.5884,\n",
      "        0.4938, 0.3960, 0.5252, 0.4498, 0.5269, 0.4257, 0.5937, 0.4814, 0.5093,\n",
      "        0.4118, 0.3862, 0.4379, 0.4449, 0.4361, 0.3274, 0.3455, 0.3234, 0.4435,\n",
      "        0.4914, 0.3558, 0.6448, 0.3840, 0.4284], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
      "tensor([0.4085, 0.4249, 0.5313, 0.4057, 0.3931, 0.4461, 0.3806, 0.3731, 0.3974,\n",
      "        0.4170, 0.4153, 0.4081, 0.4983, 0.6258, 0.5145, 0.4072, 0.4896, 0.3727,\n",
      "        0.3611, 0.4110, 0.3593, 0.3807, 0.4449, 0.3707, 0.4742, 0.4977, 0.4613,\n",
      "        0.3897, 0.3806, 0.4421, 0.3677, 0.6031], grad_fn=<CopySlices>) tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.])\n",
      "tensor([0.0185, 0.6159, 0.4249, 0.3639, 0.4001, 0.3847, 0.4154, 0.4146, 0.3177,\n",
      "        0.4250, 0.4586, 0.4060, 0.4702, 0.0068, 0.5591, 0.5393, 0.3130, 0.3281,\n",
      "        0.4099, 0.3974, 0.4072, 0.3945, 0.3465, 0.4042, 0.3678, 0.3175, 0.5664,\n",
      "        0.4055, 0.3839, 0.2644, 0.2788, 0.4854], grad_fn=<CopySlices>) tensor([0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.])\n",
      "tensor([0.5322, 0.3553, 0.4893, 0.4555, 0.3817, 0.3731, 0.5485, 0.3742, 0.4839,\n",
      "        0.4267, 0.3881, 0.4195, 0.4325, 0.3135, 0.4562, 0.4837, 0.4309, 0.4397,\n",
      "        0.3558, 0.3820, 0.3356, 0.4405, 0.3250, 0.4976, 0.3191, 0.4764, 0.3788,\n",
      "        0.3113, 0.3680, 0.2799, 0.3352, 0.4353], grad_fn=<CopySlices>) tensor([0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.])\n",
      "tensor([0.3957, 0.2942, 0.4767, 0.5174, 0.5703, 0.4102, 0.4376, 0.5671, 0.4545,\n",
      "        0.4677, 0.3474, 0.4766, 0.4765, 0.3956, 0.4870, 0.2914, 0.4486, 0.4745,\n",
      "        0.3995, 0.3950, 0.4071, 0.3269, 0.4011, 0.2620, 0.4552, 0.4348, 0.4310,\n",
      "        0.3516, 0.3336, 0.3215, 0.4303, 0.3727], grad_fn=<CopySlices>) tensor([1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.])\n",
      "tensor([0.3519, 0.3845, 0.3341, 0.2999, 0.3891, 0.5786, 0.4013, 0.4412, 0.3619,\n",
      "        0.5348, 0.5434, 0.2467, 0.4573, 0.3906, 0.3590, 0.4767, 0.4658, 0.2931,\n",
      "        0.3472, 0.3841, 0.3477, 0.3692, 0.4412, 0.2859, 0.5709, 0.3038, 0.4032,\n",
      "        0.4400, 0.4856, 0.3091, 0.4200, 0.3031], grad_fn=<CopySlices>) tensor([0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
      "tensor([0.3909, 0.1663, 0.2509, 0.3379, 0.4003, 0.4855, 0.2788, 0.2635, 0.4307,\n",
      "        0.4198, 0.2737, 0.3542, 0.4578, 0.3326, 0.2763, 0.3896, 0.3148, 0.5050,\n",
      "        0.4011, 0.3779, 0.2480, 0.4156, 0.4908, 0.3241, 0.3730, 0.3369, 0.3430,\n",
      "        0.4539, 0.3369, 0.3872, 0.3817, 0.3208], grad_fn=<CopySlices>) tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
      "tensor([0.4473, 0.2928, 0.3139, 0.2608, 0.3867, 0.2975, 0.3282, 0.4585, 0.5407,\n",
      "        0.3824, 0.3637, 0.3193, 0.4541, 0.3849, 0.2673, 0.3831, 0.3764, 0.3001,\n",
      "        0.3873, 0.2829, 0.4442, 0.4524, 0.4180, 0.3490, 0.4559, 0.4647, 0.3026,\n",
      "        0.4930, 0.3941, 0.2890, 0.4179, 0.2456], grad_fn=<CopySlices>) tensor([0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.3886, 0.2683, 0.2261, 0.2774, 0.4148, 0.3906, 0.3105, 0.2371, 0.2647,\n",
      "        0.3493, 0.2315, 0.5025, 0.2602, 0.3771, 0.4656, 0.3321, 0.2973, 0.3156,\n",
      "        0.3672, 0.4676, 0.3163, 0.3003, 0.3079, 0.3380, 0.3588, 0.2442, 0.4449,\n",
      "        0.4640, 0.2482, 0.3349, 0.4425, 0.5522], grad_fn=<CopySlices>) tensor([0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.])\n",
      "tensor([0.3388, 0.4542, 0.3276, 0.3037, 0.5214, 0.3024, 0.3029, 0.2563, 0.3056,\n",
      "        0.1872, 0.3666, 0.3774, 0.1799, 0.2275, 0.2284, 0.2862, 0.3493, 0.4554,\n",
      "        0.3155, 0.3703, 0.4221, 0.3761, 0.5283, 0.3611, 0.3277, 0.3655, 0.2866,\n",
      "        0.3474, 0.3000, 0.3296, 0.3774, 0.3479], grad_fn=<CopySlices>) tensor([1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.])\n",
      "tensor([0.3358, 0.3351, 0.2759, 0.3225, 0.4333, 0.2182, 0.4275, 0.4180, 0.3711,\n",
      "        0.3272, 0.2637, 0.3851, 0.3992, 0.3894, 0.3449, 0.3974, 0.4077, 0.3450,\n",
      "        0.2900, 0.3048, 0.3566, 0.4133, 0.2545, 0.2736, 0.2842, 0.4367, 0.3859,\n",
      "        0.2132, 0.3462, 0.3903, 0.2859, 0.4156], grad_fn=<CopySlices>) tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([0.3618, 0.3327, 0.4113, 0.3771, 0.2623, 0.3457, 0.3889, 0.2147, 0.4940,\n",
      "        0.2119, 0.2870, 0.5763, 0.2784, 0.2680, 0.3698, 0.1789, 0.4082, 0.3024,\n",
      "        0.2240, 0.4136, 0.4424, 0.2375, 0.3744, 0.2952, 0.3477, 0.2139, 0.3638,\n",
      "        0.4107, 0.3616, 0.3023, 0.3937, 0.3682], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.])\n",
      "tensor([0.3153, 0.2909, 0.2478, 0.2124, 0.2982, 0.2234, 0.2533, 0.3009, 0.3158,\n",
      "        0.3674, 0.3044, 0.1867, 0.6342, 0.0227, 0.2291, 0.2298, 0.3275, 0.3251,\n",
      "        0.3604, 0.3266, 0.4912, 0.3008, 0.5027, 0.2555, 0.2940, 0.3584, 0.3191,\n",
      "        0.2865, 0.4975, 0.3664, 0.1689, 0.2271], grad_fn=<CopySlices>) tensor([1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.])\n",
      "tensor([0.3342, 0.3309, 0.3606, 0.2776, 0.5362, 0.4784, 0.2614, 0.2065, 0.2991,\n",
      "        0.2611, 0.3251, 0.3597, 0.4063, 0.2679, 0.3663, 0.2999, 0.2556, 0.1647,\n",
      "        0.2764, 0.2550, 0.4280, 0.3322, 0.5030, 0.2795, 0.3008, 0.3146, 0.3137,\n",
      "        0.4803, 0.3605, 0.4422, 0.1496, 0.2659], grad_fn=<CopySlices>) tensor([0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.])\n",
      "tensor([0.2152, 0.2576, 0.2589, 0.4438, 0.2613, 0.4309, 0.2523, 0.2333, 0.4324,\n",
      "        0.3571, 0.1874, 0.2504, 0.2636, 0.3131, 0.3078, 0.4000, 0.2880, 0.3869,\n",
      "        0.3882, 0.2914, 0.1728, 0.4355, 0.2682, 0.4874, 0.1787, 0.3612, 0.3019,\n",
      "        0.4678, 0.2531, 0.2737, 0.3456, 0.1872], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])\n",
      "tensor([0.3166, 0.1643, 0.5201, 0.4483, 0.2553, 0.2133, 0.3832, 0.2621, 0.2931,\n",
      "        0.2360, 0.2459, 0.5268, 0.2397, 0.2650, 0.3793, 0.1395, 0.3951, 0.3717,\n",
      "        0.2710, 0.5369, 0.4488, 0.3355, 0.3750, 0.5050, 0.2126, 0.3034, 0.2767,\n",
      "        0.0126, 0.2154, 0.2979, 0.2617, 0.2507], grad_fn=<CopySlices>) tensor([0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.])\n",
      "tensor([0.1991, 0.3928, 0.3790, 0.2050, 0.2921, 0.2112, 0.3656, 0.2671, 0.3044,\n",
      "        0.3864, 0.1881, 0.4719, 0.2368, 0.1854, 0.2323, 0.6171, 0.3137, 0.3609,\n",
      "        0.2889, 0.3249, 0.3650, 0.5240, 0.1870, 0.3728, 0.2371, 0.3957, 0.3177,\n",
      "        0.3609, 0.4900, 0.3220, 0.4112, 0.4665], grad_fn=<CopySlices>) tensor([0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
      "tensor([0.3001, 0.3048, 0.3131, 0.1420, 0.2435, 0.2919, 0.3310, 0.5070, 0.3721,\n",
      "        0.4738, 0.2726, 0.4149, 0.3402, 0.3238, 0.2694, 0.3646, 0.4184, 0.2526,\n",
      "        0.4501, 0.2405, 0.2954, 0.4059, 0.2993, 0.2083, 0.2946, 0.2766, 0.1310,\n",
      "        0.3257, 0.3058, 0.3003, 0.2214, 0.2571], grad_fn=<CopySlices>) tensor([0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.])\n",
      "tensor([0.2283, 0.3583, 0.3170, 0.5453, 0.2195, 0.3645, 0.4869, 0.2426, 0.3449,\n",
      "        0.2367, 0.2487, 0.2611, 0.3922, 0.2184, 0.2993, 0.3308, 0.2547, 0.1824,\n",
      "        0.3524, 0.3621, 0.5198, 0.3416, 0.3763, 0.3019, 0.2280, 0.2802, 0.4472,\n",
      "        0.3871, 0.2256, 0.2539, 0.3082, 0.3978], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.])\n",
      "tensor([0.4421, 0.3295, 0.2938, 0.3180, 0.2170, 0.3573, 0.4993, 0.4273, 0.4138,\n",
      "        0.5156, 0.4206, 0.2948, 0.2866, 0.3662, 0.2737, 0.3739, 0.2912, 0.3020,\n",
      "        0.3895, 0.2860, 0.2811, 0.3372, 0.4059, 0.2822, 0.3095, 0.4628, 0.2298,\n",
      "        0.3296, 0.4974, 0.3190, 0.2725, 0.3263], grad_fn=<CopySlices>) tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.])\n",
      "tensor([0.2106, 0.3918, 0.2530, 0.4123, 0.2344, 0.4980, 0.4721, 0.3625, 0.5420,\n",
      "        0.3477, 0.4285, 0.3543, 0.3822, 0.0954, 0.2753, 0.3171, 0.1501, 0.4479,\n",
      "        0.3000, 0.2812, 0.2898, 0.3374, 0.2041, 0.3407, 0.3424, 0.3381, 0.3112,\n",
      "        0.3711, 0.4835, 0.2222, 0.4471, 0.1750], grad_fn=<CopySlices>) tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([0.5536, 0.2692, 0.2725, 0.4400, 0.4163, 0.1971, 0.3528, 0.4484, 0.4984,\n",
      "        0.5583, 0.3970, 0.4973, 0.4375, 0.2613, 0.3078, 0.3773, 0.3760, 0.4465,\n",
      "        0.3783, 0.3150, 0.4063, 0.5573, 0.6205, 0.2655, 0.2909, 0.5567, 0.4786,\n",
      "        0.3171, 0.3237, 0.4654, 0.4252, 0.3155], grad_fn=<CopySlices>) tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.])\n",
      "tensor([0.2224, 0.3823, 0.3401, 0.3503, 0.2034, 0.3401, 0.2194, 0.3429, 0.3134,\n",
      "        0.2370, 0.4213, 0.2945, 0.4020, 0.2219, 0.4594, 0.3937, 0.2944, 0.3050,\n",
      "        0.4007, 0.3211, 0.3393, 0.1988, 0.4046, 0.1166, 0.2733, 0.1932, 0.5467,\n",
      "        0.3673, 0.3629, 0.1899, 0.4282, 0.3094], grad_fn=<CopySlices>) tensor([0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([0.3140, 0.2358, 0.3305, 0.1709, 0.4897, 0.2060, 0.3493, 0.4975, 0.4152,\n",
      "        0.3126, 0.2083, 0.2202, 0.4970, 0.4102, 0.3841, 0.4891, 0.2678, 0.3657,\n",
      "        0.3050, 0.3966, 0.3084, 0.3346, 0.4542, 0.4634, 0.4409, 0.1496, 0.3246,\n",
      "        0.2560, 0.4721, 0.2953, 0.1538, 0.3536], grad_fn=<CopySlices>) tensor([1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.1302, 0.3844, 0.3664, 0.3604, 0.3628, 0.2922, 0.3696, 0.2239, 0.4287,\n",
      "        0.3985, 0.3257, 0.2565, 0.2625, 0.3831, 0.1863, 0.2834, 0.2527, 0.3137,\n",
      "        0.3687, 0.2647, 0.4127, 0.3029, 0.4340, 0.2436, 0.2958, 0.4028, 0.2715,\n",
      "        0.3818, 0.4197, 0.2703, 0.4304, 0.2662], grad_fn=<CopySlices>) tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.])\n",
      "tensor([0.5756, 0.2666, 0.5749, 0.3357, 0.3231, 0.3545, 0.2674, 0.2434, 0.3934,\n",
      "        0.2809, 0.3487, 0.2969, 0.3631, 0.2148, 0.3493, 0.3813, 0.3374, 0.2811,\n",
      "        0.3457, 0.3764, 0.2817, 0.2394, 0.5006, 0.3586, 0.1653, 0.4612, 0.3141,\n",
      "        0.2620, 0.2752, 0.1695, 0.3140, 0.4656], grad_fn=<CopySlices>) tensor([1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0.])\n",
      "tensor([0.2694, 0.3706, 0.3635, 0.1256, 0.3663, 0.4670, 0.3689, 0.2431, 0.1108,\n",
      "        0.3253, 0.3737, 0.3878, 0.2864, 0.5920, 0.2459, 0.1802, 0.2726, 0.2671,\n",
      "        0.2246, 0.3735, 0.5045, 0.4975, 0.3721, 0.3517, 0.3632, 0.5114, 0.3846,\n",
      "        0.3901, 0.2212, 0.2211, 0.2298, 0.3640], grad_fn=<CopySlices>) tensor([1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4895, 0.3362, 0.2600, 0.3009, 0.1839, 0.3715, 0.3754, 0.3496, 0.3095,\n",
      "        0.3325, 0.2934, 0.5095, 0.3335, 0.2313, 0.3029, 0.2709, 0.2995, 0.2393,\n",
      "        0.3198, 0.3133, 0.4587, 0.4694, 0.4131, 0.5125, 0.3607, 0.3458, 0.3614,\n",
      "        0.2950, 0.4058, 0.2873, 0.3627, 0.1993], grad_fn=<CopySlices>) tensor([1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.])\n",
      "tensor([0.2261, 0.5983, 0.4531, 0.6628, 0.4899, 0.1866, 0.3702, 0.2338, 0.5939,\n",
      "        0.4360, 0.2809, 0.3845, 0.2926, 0.3421, 0.3933, 0.3609, 0.3787, 0.3171,\n",
      "        0.4027, 0.2857, 0.2369, 0.3716, 0.3811, 0.4035, 0.3812, 0.3093, 0.3801,\n",
      "        0.2327, 0.3345, 0.4007, 0.3037, 0.3212], grad_fn=<CopySlices>) tensor([0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "tensor([0.2351, 0.3796, 0.3807, 0.3156, 0.4740, 0.2461, 0.2940, 0.3370, 0.4032,\n",
      "        0.4113, 0.2439, 0.2397, 0.4967, 0.3672, 0.2704, 0.3052, 0.2402, 0.3030,\n",
      "        0.4652, 0.4440, 0.4452, 0.3311, 0.3604, 0.2159, 0.5241, 0.4402, 0.3888,\n",
      "        0.2929, 0.2802, 0.1760, 0.4269, 0.4400], grad_fn=<CopySlices>) tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([0.2184, 0.2343, 0.3706, 0.4291, 0.2477, 0.2350, 0.2910, 0.2943, 0.3720,\n",
      "        0.3055, 0.4956, 0.3925, 0.3472, 0.4829, 0.2342, 0.1610, 0.2354, 0.1985,\n",
      "        0.3958, 0.4995, 0.3381, 0.6294, 0.5001, 0.3007, 0.2612, 0.4914, 0.2985,\n",
      "        0.2981, 0.3142, 0.3492, 0.2476, 0.4247], grad_fn=<CopySlices>) tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
      "tensor([0.3533, 0.2478, 0.2959, 0.3473, 0.5440, 0.3768, 0.5408, 0.2938, 0.3898,\n",
      "        0.4250, 0.3873, 0.5132, 0.2678, 0.2841, 0.5683, 0.1950, 0.5599, 0.3141,\n",
      "        0.2186, 0.3850, 0.2794, 0.3365, 0.2438, 0.3133, 0.3399, 0.4017, 0.2857,\n",
      "        0.3244, 0.4300, 0.0280, 0.4543, 0.3800], grad_fn=<CopySlices>) tensor([0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.])\n",
      "tensor([0.4328, 0.3789, 0.3968, 0.5810, 0.2046, 0.3358, 0.3283, 0.3477, 0.4117,\n",
      "        0.4340, 0.3542, 0.4933, 0.2985, 0.4748, 0.1111, 0.2662, 0.3620, 0.3094,\n",
      "        0.2756, 0.3685, 0.2471, 0.3412, 0.4122, 0.3544, 0.3334, 0.2184, 0.2914,\n",
      "        0.3981, 0.2940, 0.2562, 0.3311, 0.4487], grad_fn=<CopySlices>) tensor([0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
      "tensor([0.4431, 0.5200, 0.4699, 0.4535, 0.2105, 0.3982, 0.2010, 0.6050, 0.3915,\n",
      "        0.3470, 0.3549, 0.3597, 0.3420, 0.1726, 0.2572, 0.4262, 0.2927, 0.3765,\n",
      "        0.2818, 0.3381, 0.2487, 0.2629, 0.3846, 0.4824, 0.2981, 0.4937, 0.2422,\n",
      "        0.3599, 0.4868, 0.3937, 0.3324, 0.2842], grad_fn=<CopySlices>) tensor([0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.])\n",
      "tensor([0.2427, 0.2988, 0.2222, 0.3912, 0.4199, 0.2864, 0.3444, 0.3952, 0.3109,\n",
      "        0.4309, 0.2230, 0.2369, 0.2642, 0.2076, 0.1530, 0.4384, 0.3903, 0.2685,\n",
      "        0.3700, 0.3466, 0.3733, 0.3616, 0.3808, 0.4901, 0.2447, 0.2947, 0.4938,\n",
      "        0.1621, 0.3170, 0.3943, 0.2658, 0.5125], grad_fn=<CopySlices>) tensor([1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.])\n",
      "tensor([0.3847, 0.2707, 0.5792, 0.3638, 0.3525, 0.3269, 0.1732, 0.3524, 0.3051,\n",
      "        0.2843, 0.2310, 0.2681, 0.2363, 0.4472, 0.3381, 0.4028, 0.2998, 0.2583,\n",
      "        0.4252, 0.2857, 0.3166, 0.2865, 0.3286, 0.2667, 0.2433, 0.1714, 0.4588,\n",
      "        0.4145, 0.2004, 0.5632, 0.2629, 0.4525], grad_fn=<CopySlices>) tensor([0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.])\n",
      "tensor([0.2405, 0.5494, 0.4257, 0.3901, 0.3925, 0.5157, 0.3194, 0.3641, 0.2906,\n",
      "        0.3652, 0.4863, 0.4124, 0.1589, 0.2519, 0.2406, 0.2790, 0.1880, 0.3218,\n",
      "        0.3350, 0.5297, 0.3012, 0.3257, 0.3483, 0.3474, 0.2936, 0.4186, 0.3180,\n",
      "        0.1726, 0.3317, 0.3264, 0.4765, 0.4610], grad_fn=<CopySlices>) tensor([1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.])\n",
      "tensor([0.3853, 0.4834, 0.1569, 0.3378, 0.2269, 0.5223, 0.4076, 0.4000, 0.4338,\n",
      "        0.3301, 0.6242, 0.4173, 0.2589, 0.2620, 0.2109, 0.2287, 0.3636, 0.2052,\n",
      "        0.3225, 0.3848, 0.3854, 0.1925, 0.4209, 0.3140, 0.3982, 0.5486, 0.6687,\n",
      "        0.3859, 0.4193, 0.3498, 0.4257, 0.3433], grad_fn=<CopySlices>) tensor([1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([0.2956, 0.3847, 0.4101, 0.4886, 0.2068, 0.2989, 0.4348, 0.2329, 0.3878,\n",
      "        0.3232, 0.5370, 0.2917, 0.2247, 0.5782, 0.2808, 0.1744, 0.4115, 0.3469,\n",
      "        0.3204, 0.2558, 0.3432, 0.3360, 0.3274, 0.4475, 0.3680, 0.5880, 0.4547,\n",
      "        0.3635, 0.3206, 0.3839, 0.4304, 0.3461], grad_fn=<CopySlices>) tensor([0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([0.4110, 0.2363, 0.3554, 0.5354, 0.6208, 0.3637, 0.5393, 0.2919, 0.1527,\n",
      "        0.3896, 0.2963, 0.1844, 0.3518, 0.5157, 0.5352, 0.1510, 0.1565, 0.2783,\n",
      "        0.3035, 0.2875, 0.3390, 0.4671, 0.2168, 0.4689, 0.3169, 0.4264, 0.3767,\n",
      "        0.2416, 0.3613, 0.5844, 0.4758, 0.1818], grad_fn=<CopySlices>) tensor([1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.])\n",
      "tensor([0.2941, 0.1732, 0.5283, 0.3258, 0.2768, 0.3289, 0.3059, 0.1803, 0.5006,\n",
      "        0.2162, 0.3726, 0.4120, 0.3573, 0.3301, 0.3091, 0.2891, 0.1850, 0.4631,\n",
      "        0.3657, 0.2306, 0.2384, 0.1922, 0.2518, 0.4697, 0.2842, 0.2636, 0.3834,\n",
      "        0.3292, 0.2781, 0.3863, 0.2214, 0.2237], grad_fn=<CopySlices>) tensor([1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4893, 0.5079, 0.2401, 0.2209, 0.4043, 0.5729, 0.2289, 0.6550, 0.2983,\n",
      "        0.3163, 0.3975, 0.5451, 0.2387, 0.2190, 0.6007, 0.2463, 0.4654, 0.3636,\n",
      "        0.3077, 0.2901, 0.1953, 0.5564, 0.5026, 0.5827, 0.4587, 0.5009, 0.3772,\n",
      "        0.3515, 0.2800, 0.2194, 0.3002, 0.5570], grad_fn=<CopySlices>) tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.])\n",
      "tensor([0.2052, 0.4994, 0.4440, 0.1383, 0.4959, 0.4006, 0.3141, 0.3317, 0.4423,\n",
      "        0.2893, 0.1834, 0.3072, 0.1517, 0.5678, 0.2564, 0.2516, 0.2239, 0.1404,\n",
      "        0.2998, 0.5200, 0.4598, 0.4928, 0.3862, 0.3975, 0.2745, 0.2539, 0.3603,\n",
      "        0.3970, 0.5239, 0.5169, 0.3280, 0.2772], grad_fn=<CopySlices>) tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.])\n",
      "tensor([0.3865, 0.3903, 0.2005, 0.2501, 0.6210, 0.2002, 0.1818, 0.2996, 0.3676,\n",
      "        0.4001, 0.2178, 0.2568, 0.3453, 0.3425, 0.3463, 0.4038, 0.3563, 0.2237,\n",
      "        0.2899, 0.3028, 0.3399, 0.2633, 0.2409, 0.2181, 0.3176, 0.1682, 0.2922,\n",
      "        0.3518, 0.2760, 0.2279, 0.3615, 0.4118], grad_fn=<CopySlices>) tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.])\n",
      "tensor([0.5374, 0.4237, 0.4644, 0.4262, 0.2434, 0.4934, 0.1367, 0.4543, 0.4445,\n",
      "        0.2449, 0.1030, 0.2564, 0.2430, 0.3397, 0.4424, 0.1582, 0.2244, 0.2653,\n",
      "        0.3400, 0.4271, 0.3322, 0.2808, 0.2182, 0.2839, 0.4711, 0.2728, 0.4098,\n",
      "        0.2810, 0.4102, 0.2446, 0.2816, 0.2121], grad_fn=<CopySlices>) tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0.])\n",
      "tensor([0.1459, 0.2650, 0.3425, 0.2609, 0.5077, 0.4035, 0.3832, 0.3189, 0.1979,\n",
      "        0.2520, 0.2625, 0.3193, 0.3191, 0.4578, 0.4694, 0.2371, 0.2954, 0.4276,\n",
      "        0.3672, 0.1978, 0.2838, 0.3091, 0.3577, 0.4850, 0.5249, 0.4008, 0.5830,\n",
      "        0.2614, 0.2165, 0.3349, 0.2400, 0.1706], grad_fn=<CopySlices>) tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "tensor([0.1892, 0.0067, 0.4277, 0.4015, 0.4375, 0.2004, 0.2532, 0.3009, 0.3286,\n",
      "        0.2619, 0.3535, 0.1904, 0.4104, 0.5017, 0.3040, 0.3035, 0.4998, 0.1995,\n",
      "        0.3556, 0.4426, 0.2018, 0.5019, 0.2760, 0.3100, 0.2755, 0.4769, 0.4181,\n",
      "        0.2894, 0.4497, 0.1817, 0.4557, 0.4141], grad_fn=<CopySlices>) tensor([0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1.])\n",
      "tensor([0.3112, 0.3369, 0.2412, 0.3673, 0.4025, 0.2201, 0.1988, 0.3581, 0.1254,\n",
      "        0.3497, 0.2511, 0.2774, 0.3580, 0.2142, 0.1429, 0.4722, 0.4185, 0.1968,\n",
      "        0.0333, 0.1960, 0.4132, 0.4939, 0.3401, 0.1705, 0.2604, 0.2868, 0.3151,\n",
      "        0.1624, 0.1408, 0.3387, 0.3036, 0.2165], grad_fn=<CopySlices>) tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.])\n",
      "tensor([0.2606, 0.2447, 0.2857, 0.6904, 0.3719, 0.5055, 0.0826, 0.2637, 0.3507,\n",
      "        0.5614, 0.3402, 0.2897, 0.2961, 0.2321, 0.2616, 0.2251, 0.3136, 0.2537,\n",
      "        0.1618, 0.3892, 0.2717, 0.2613, 0.2974, 0.5106, 0.4565, 0.4551, 0.3032,\n",
      "        0.3730, 0.2865, 0.3710, 0.2447, 0.3301], grad_fn=<CopySlices>) tensor([0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.])\n",
      "tensor([0.1474, 0.5110, 0.4294, 0.1687, 0.2341, 0.4943, 0.5899, 0.3259, 0.2359,\n",
      "        0.3523, 0.2277, 0.1333, 0.1982, 0.4490, 0.4283, 0.4115],\n",
      "       grad_fn=<CopySlices>) tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.])\n",
      "[0.23418608922806997]\n",
      "1\n",
      "tensor([0.0076, 0.1585, 0.2630, 0.1917, 0.1722, 0.3121, 0.2646, 0.1655, 0.2556,\n",
      "        0.1961, 0.1487, 0.3525, 0.2791, 0.3044, 0.2270, 0.1664, 0.4717, 0.2005,\n",
      "        0.2593, 0.2677, 0.2802, 0.2494, 0.4713, 0.1851, 0.1596, 0.3719, 0.4298,\n",
      "        0.0910, 0.2321, 0.1340, 0.2269, 0.1789], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.])\n",
      "tensor([0.3052, 0.3202, 0.2222, 0.2879, 0.2658, 0.2450, 0.2969, 0.2686, 0.2056,\n",
      "        0.2192, 0.3079, 0.3717, 0.5446, 0.4153, 0.1951, 0.4204, 0.2457, 0.5982,\n",
      "        0.2083, 0.2977, 0.2490, 0.5017, 0.2530, 0.3860, 0.3243, 0.2287, 0.3410,\n",
      "        0.2823, 0.3234, 0.1656, 0.3143, 0.1985], grad_fn=<CopySlices>) tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.])\n",
      "tensor([0.1359, 0.3071, 0.1986, 0.3458, 0.1865, 0.2321, 0.1739, 0.3306, 0.2321,\n",
      "        0.4207, 0.2282, 0.3073, 0.1494, 0.1643, 0.2288, 0.3269, 0.2554, 0.1868,\n",
      "        0.1461, 0.2387, 0.1814, 0.4909, 0.4332, 0.4180, 0.2014, 0.1823, 0.1906,\n",
      "        0.2794, 0.3625, 0.4134, 0.2085, 0.3838], grad_fn=<CopySlices>) tensor([0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.])\n",
      "tensor([0.2480, 0.4607, 0.3598, 0.3286, 0.2834, 0.2295, 0.4224, 0.2876, 0.4106,\n",
      "        0.4196, 0.2323, 0.4336, 0.3041, 0.0031, 0.4599, 0.2363, 0.3615, 0.2912,\n",
      "        0.3600, 0.3866, 0.2254, 0.2332, 0.4575, 0.3197, 0.3851, 0.3907, 0.2507,\n",
      "        0.2372, 0.3339, 0.1785, 0.3167, 0.3969], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1.])\n",
      "tensor([0.2453, 0.4992, 0.4789, 0.2116, 0.2915, 0.3208, 0.4353, 0.5074, 0.3276,\n",
      "        0.2488, 0.3504, 0.3412, 0.4632, 0.3011, 0.2581, 0.4105, 0.3983, 0.3010,\n",
      "        0.4008, 0.2787, 0.3008, 0.2033, 0.2677, 0.3629, 0.3361, 0.4011, 0.2419,\n",
      "        0.2403, 0.0313, 0.4297, 0.2346, 0.4752], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.])\n",
      "tensor([0.2837, 0.4033, 0.3314, 0.3701, 0.2347, 0.3188, 0.2174, 0.2997, 0.2539,\n",
      "        0.2624, 0.2831, 0.2613, 0.2679, 0.4266, 0.2958, 0.3246, 0.4586, 0.3644,\n",
      "        0.2361, 0.4228, 0.3156, 0.3161, 0.3552, 0.2286, 0.2309, 0.3688, 0.1351,\n",
      "        0.3324, 0.5478, 0.3774, 0.4971, 0.2822], grad_fn=<CopySlices>) tensor([1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.])\n",
      "tensor([0.2580, 0.2890, 0.2856, 0.4607, 0.2686, 0.3110, 0.3150, 0.1664, 0.4189,\n",
      "        0.2753, 0.3259, 0.3267, 0.2895, 0.2474, 0.3262, 0.4109, 0.3703, 0.4347,\n",
      "        0.2794, 0.1996, 0.3148, 0.3506, 0.3853, 0.3417, 0.3436, 0.2391, 0.3390,\n",
      "        0.3872, 0.3478, 0.3733, 0.2184, 0.2696], grad_fn=<CopySlices>) tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.])\n",
      "tensor([0.3583, 0.3092, 0.3856, 0.3964, 0.2203, 0.2859, 0.5763, 0.2671, 0.4229,\n",
      "        0.3249, 0.1675, 0.4648, 0.3198, 0.4075, 0.1527, 0.3843, 0.2446, 0.2017,\n",
      "        0.2883, 0.2514, 0.4198, 0.4035, 0.2170, 0.2704, 0.4070, 0.5384, 0.2939,\n",
      "        0.1953, 0.2627, 0.4360, 0.2887, 0.4086], grad_fn=<CopySlices>) tensor([1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.])\n",
      "tensor([0.2671, 0.3639, 0.3919, 0.3697, 0.3557, 0.3784, 0.4705, 0.2790, 0.1392,\n",
      "        0.3153, 0.4750, 0.4392, 0.3857, 0.4165, 0.3314, 0.3516, 0.4571, 0.4859,\n",
      "        0.5409, 0.3505, 0.3660, 0.4020, 0.1694, 0.3158, 0.3095, 0.2507, 0.3660,\n",
      "        0.3772, 0.3918, 0.2716, 0.3620, 0.4739], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.])\n",
      "tensor([0.3909, 0.2458, 0.5529, 0.3986, 0.2607, 0.5232, 0.3295, 0.3023, 0.2436,\n",
      "        0.2475, 0.4706, 0.4052, 0.4234, 0.2651, 0.5881, 0.3854, 0.3270, 0.4239,\n",
      "        0.3532, 0.4683, 0.3366, 0.3950, 0.2684, 0.4082, 0.5531, 0.3299, 0.3168,\n",
      "        0.3994, 0.2681, 0.4969, 0.4720, 0.3259], grad_fn=<CopySlices>) tensor([1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.])\n",
      "tensor([0.2591, 0.3417, 0.3841, 0.3947, 0.4556, 0.3171, 0.4367, 0.2692, 0.3234,\n",
      "        0.3585, 0.3784, 0.2738, 0.2152, 0.0841, 0.2962, 0.3451, 0.1658, 0.3685,\n",
      "        0.3834, 0.4189, 0.3983, 0.4072, 0.3236, 0.4370, 0.3421, 0.2056, 0.4650,\n",
      "        0.2902, 0.5421, 0.5538, 0.2605, 0.2797], grad_fn=<CopySlices>) tensor([0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.])\n",
      "tensor([0.4075, 0.3019, 0.4377, 0.0188, 0.5718, 0.2844, 0.3815, 0.3212, 0.3883,\n",
      "        0.3711, 0.5729, 0.5773, 0.4949, 0.5855, 0.4086, 0.1942, 0.5019, 0.4337,\n",
      "        0.2899, 0.2722, 0.5845, 0.2345, 0.3262, 0.3877, 0.3727, 0.4416, 0.2805,\n",
      "        0.3480, 0.4972, 0.6284, 0.3538, 0.5578], grad_fn=<CopySlices>) tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.])\n",
      "tensor([0.4933, 0.4143, 0.3276, 0.1791, 0.4840, 0.2933, 0.5216, 0.3101, 0.3068,\n",
      "        0.3183, 0.5410, 0.4383, 0.4510, 0.5134, 0.3614, 0.5661, 0.4543, 0.3362,\n",
      "        0.5662, 0.4106, 0.3497, 0.3516, 0.2324, 0.3822, 0.5914, 0.3729, 0.3404,\n",
      "        0.3884, 0.5396, 0.3277, 0.2449, 0.4861], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
      "tensor([0.4492, 0.3865, 0.3306, 0.4946, 0.3099, 0.4687, 0.4182, 0.6062, 0.2495,\n",
      "        0.2665, 0.3337, 0.2922, 0.4150, 0.5097, 0.4361, 0.6070, 0.5148, 0.3393,\n",
      "        0.3369, 0.3954, 0.3500, 0.4554, 0.4598, 0.4775, 0.2510, 0.2413, 0.3912,\n",
      "        0.5932, 0.5607, 0.5995, 0.4087, 0.2864], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
      "tensor([0.4218, 0.4543, 0.4521, 0.3196, 0.3648, 0.3899, 0.3219, 0.5421, 0.3699,\n",
      "        0.3933, 0.4544, 0.4606, 0.6413, 0.6036, 0.5549, 0.6186, 0.4871, 0.3410,\n",
      "        0.4009, 0.3428, 0.2024, 0.4086, 0.2594, 0.3379, 0.2941, 0.2812, 0.3478,\n",
      "        0.3839, 0.3498, 0.3419, 0.5421, 0.5833], grad_fn=<CopySlices>) tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.])\n",
      "tensor([0.0114, 0.5460, 0.4654, 0.3195, 0.5296, 0.3220, 0.5163, 0.6123, 0.3168,\n",
      "        0.5778, 0.1862, 0.5118, 0.6073, 0.0102, 0.3969, 0.6398, 0.4294, 0.3002,\n",
      "        0.3288, 0.3532, 0.3853, 0.4702, 0.3630, 0.4667, 0.2297, 0.3242, 0.5595,\n",
      "        0.3771, 0.5118, 0.3399, 0.2614, 0.5415], grad_fn=<CopySlices>) tensor([0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.])\n",
      "tensor([0.5947, 0.2911, 0.4659, 0.4236, 0.5544, 0.4565, 0.4277, 0.4243, 0.4186,\n",
      "        0.2633, 0.4128, 0.4235, 0.5438, 0.3075, 0.2825, 0.3264, 0.4168, 0.3519,\n",
      "        0.3327, 0.3313, 0.5171, 0.5313, 0.4646, 0.5699, 0.4165, 0.3195, 0.3253,\n",
      "        0.2711, 0.3354, 0.3206, 0.3818, 0.4030], grad_fn=<CopySlices>) tensor([0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.])\n",
      "tensor([0.3304, 0.3904, 0.2863, 0.5154, 0.3250, 0.4037, 0.4115, 0.5348, 0.3104,\n",
      "        0.2362, 0.3999, 0.5287, 0.5206, 0.4943, 0.5242, 0.3972, 0.3042, 0.3522,\n",
      "        0.3089, 0.6351, 0.3934, 0.2854, 0.3975, 0.5034, 0.5009, 0.2977, 0.3601,\n",
      "        0.3398, 0.4061, 0.3264, 0.5609, 0.3809], grad_fn=<CopySlices>) tensor([1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.])\n",
      "tensor([0.2712, 0.5338, 0.3210, 0.3112, 0.6104, 0.3516, 0.3918, 0.5183, 0.4618,\n",
      "        0.6939, 0.5123, 0.2708, 0.4214, 0.3627, 0.3420, 0.5472, 0.4429, 0.2083,\n",
      "        0.2454, 0.4830, 0.3848, 0.4187, 0.2791, 0.3143, 0.6195, 0.4158, 0.4857,\n",
      "        0.4771, 0.2679, 0.2958, 0.4087, 0.3161], grad_fn=<CopySlices>) tensor([0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
      "tensor([0.5531, 0.2085, 0.3043, 0.3982, 0.4191, 0.3606, 0.2590, 0.3401, 0.4197,\n",
      "        0.5791, 0.4087, 0.3240, 0.6015, 0.3318, 0.2666, 0.5685, 0.1829, 0.5242,\n",
      "        0.3037, 0.3438, 0.4451, 0.4656, 0.4725, 0.3069, 0.3241, 0.4161, 0.4583,\n",
      "        0.3733, 0.3846, 0.3879, 0.2821, 0.4361], grad_fn=<CopySlices>) tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
      "tensor([0.6029, 0.4081, 0.2643, 0.3640, 0.3691, 0.4272, 0.4771, 0.5883, 0.4575,\n",
      "        0.3288, 0.3767, 0.2655, 0.4495, 0.3442, 0.4593, 0.6460, 0.3789, 0.3342,\n",
      "        0.4366, 0.2618, 0.5452, 0.5401, 0.5600, 0.3410, 0.3117, 0.3237, 0.1138,\n",
      "        0.4526, 0.3805, 0.3657, 0.3864, 0.1165], grad_fn=<CopySlices>) tensor([0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4085, 0.2365, 0.3576, 0.5019, 0.4910, 0.4525, 0.4311, 0.2620, 0.4513,\n",
      "        0.2858, 0.3087, 0.6455, 0.3234, 0.3436, 0.6042, 0.1613, 0.3021, 0.3953,\n",
      "        0.3294, 0.3821, 0.2088, 0.1700, 0.2537, 0.3545, 0.4718, 0.3065, 0.5297,\n",
      "        0.4289, 0.5166, 0.4009, 0.5665, 0.5082], grad_fn=<CopySlices>) tensor([0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.])\n",
      "tensor([0.5213, 0.5428, 0.3275, 0.3673, 0.2232, 0.3810, 0.2074, 0.2740, 0.2617,\n",
      "        0.2102, 0.4487, 0.4751, 0.3471, 0.2173, 0.2410, 0.2734, 0.5144, 0.4431,\n",
      "        0.5001, 0.3306, 0.4340, 0.4180, 0.4244, 0.4183, 0.4741, 0.4803, 0.2956,\n",
      "        0.2795, 0.4036, 0.3266, 0.4317, 0.2798], grad_fn=<CopySlices>) tensor([1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.])\n",
      "tensor([0.2753, 0.4358, 0.3327, 0.2615, 0.4654, 0.3470, 0.6497, 0.2982, 0.2616,\n",
      "        0.4005, 0.2935, 0.2446, 0.4012, 0.5897, 0.4915, 0.2461, 0.5114, 0.3994,\n",
      "        0.1839, 0.3888, 0.3335, 0.4927, 0.1651, 0.3397, 0.4704, 0.5963, 0.3480,\n",
      "        0.1820, 0.4423, 0.3714, 0.3979, 0.1706], grad_fn=<CopySlices>) tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([0.3305, 0.5714, 0.3880, 0.4143, 0.3102, 0.5546, 0.3554, 0.2414, 0.4622,\n",
      "        0.2845, 0.3019, 0.4409, 0.2381, 0.1751, 0.3897, 0.1523, 0.4842, 0.3484,\n",
      "        0.3484, 0.3270, 0.5764, 0.3082, 0.3341, 0.2688, 0.3862, 0.1908, 0.3364,\n",
      "        0.4497, 0.3651, 0.3673, 0.6029, 0.3053], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.])\n",
      "tensor([0.4235, 0.3124, 0.3410, 0.3184, 0.2940, 0.1957, 0.3984, 0.3973, 0.2500,\n",
      "        0.3954, 0.2844, 0.1242, 0.5442, 0.0118, 0.3312, 0.3651, 0.4121, 0.3662,\n",
      "        0.4637, 0.4892, 0.3810, 0.3185, 0.6361, 0.3230, 0.2659, 0.4166, 0.2204,\n",
      "        0.4669, 0.4575, 0.2754, 0.2305, 0.3920], grad_fn=<CopySlices>) tensor([1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.])\n",
      "tensor([0.2287, 0.2595, 0.2537, 0.2633, 0.4713, 0.5370, 0.2868, 0.2617, 0.3607,\n",
      "        0.3648, 0.3617, 0.2166, 0.2532, 0.1304, 0.2791, 0.2563, 0.2775, 0.2710,\n",
      "        0.2862, 0.2938, 0.4532, 0.2869, 0.5149, 0.1836, 0.4347, 0.3201, 0.4861,\n",
      "        0.5248, 0.2215, 0.5544, 0.1921, 0.5325], grad_fn=<CopySlices>) tensor([0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.])\n",
      "tensor([0.1284, 0.0944, 0.1329, 0.4282, 0.3727, 0.2552, 0.3090, 0.3029, 0.5946,\n",
      "        0.5210, 0.2400, 0.2334, 0.2474, 0.1973, 0.1570, 0.5969, 0.2359, 0.3591,\n",
      "        0.4054, 0.4059, 0.3374, 0.6666, 0.1963, 0.2223, 0.1682, 0.3519, 0.5973,\n",
      "        0.5733, 0.3443, 0.4035, 0.3531, 0.1705], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])\n",
      "tensor([0.3201, 0.0763, 0.6106, 0.4819, 0.2317, 0.1790, 0.2670, 0.3145, 0.2533,\n",
      "        0.3801, 0.2453, 0.6087, 0.3846, 0.3280, 0.4919, 0.1839, 0.5407, 0.2276,\n",
      "        0.3973, 0.4994, 0.4529, 0.4272, 0.3292, 0.4499, 0.2381, 0.3051, 0.3726,\n",
      "        0.0084, 0.1736, 0.3107, 0.2830, 0.3316], grad_fn=<CopySlices>) tensor([0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.])\n",
      "tensor([0.1792, 0.6118, 0.3532, 0.1607, 0.3206, 0.2211, 0.5707, 0.2036, 0.2173,\n",
      "        0.3082, 0.1573, 0.3301, 0.2769, 0.3116, 0.2862, 0.5014, 0.2239, 0.4191,\n",
      "        0.4180, 0.3714, 0.2469, 0.4803, 0.1844, 0.4084, 0.2757, 0.3716, 0.2753,\n",
      "        0.3289, 0.5078, 0.1715, 0.4953, 0.5650], grad_fn=<CopySlices>) tensor([0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
      "tensor([0.3650, 0.2355, 0.1900, 0.2365, 0.2829, 0.1857, 0.3850, 0.5684, 0.5699,\n",
      "        0.3565, 0.3383, 0.4501, 0.3130, 0.4419, 0.2636, 0.3271, 0.5262, 0.2748,\n",
      "        0.4809, 0.2856, 0.1875, 0.2983, 0.3748, 0.5295, 0.3451, 0.3718, 0.2153,\n",
      "        0.1713, 0.3095, 0.4605, 0.0949, 0.3119], grad_fn=<CopySlices>) tensor([0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.])\n",
      "tensor([0.2963, 0.5135, 0.2692, 0.4723, 0.2068, 0.2279, 0.5471, 0.2369, 0.1905,\n",
      "        0.5407, 0.2270, 0.4215, 0.3701, 0.4396, 0.1901, 0.2129, 0.2858, 0.2504,\n",
      "        0.3091, 0.3453, 0.3296, 0.3386, 0.2842, 0.3660, 0.2276, 0.2677, 0.4087,\n",
      "        0.1886, 0.3878, 0.2296, 0.2473, 0.4048], grad_fn=<CopySlices>) tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.])\n",
      "tensor([0.5346, 0.3532, 0.2438, 0.2517, 0.3814, 0.4274, 0.4782, 0.3675, 0.1997,\n",
      "        0.5156, 0.2526, 0.2706, 0.3504, 0.3660, 0.2465, 0.4035, 0.2568, 0.2684,\n",
      "        0.5904, 0.4121, 0.3057, 0.2242, 0.3061, 0.5645, 0.4667, 0.4780, 0.2034,\n",
      "        0.4676, 0.4397, 0.3226, 0.4976, 0.3208], grad_fn=<CopySlices>) tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.])\n",
      "tensor([0.1924, 0.6024, 0.2490, 0.2870, 0.2582, 0.3744, 0.5923, 0.1899, 0.5662,\n",
      "        0.2346, 0.3915, 0.1649, 0.3487, 0.1887, 0.3459, 0.2891, 0.1778, 0.5580,\n",
      "        0.2832, 0.3275, 0.1760, 0.3378, 0.2414, 0.1963, 0.2591, 0.2634, 0.2729,\n",
      "        0.2407, 0.4581, 0.4416, 0.4440, 0.2325], grad_fn=<CopySlices>) tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([0.4701, 0.3057, 0.2084, 0.4214, 0.2754, 0.1887, 0.2733, 0.2418, 0.5041,\n",
      "        0.5149, 0.1407, 0.3202, 0.4736, 0.1813, 0.2724, 0.2403, 0.2889, 0.4938,\n",
      "        0.4313, 0.4690, 0.5778, 0.4437, 0.5107, 0.1517, 0.3251, 0.4723, 0.5523,\n",
      "        0.2984, 0.3583, 0.4322, 0.4459, 0.3727], grad_fn=<CopySlices>) tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m y\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# y[y==0]=-1\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m similarity_score\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq1_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq2_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(similarity_score, y)\n\u001b[1;32m     87\u001b[0m loss\u001b[38;5;241m=\u001b[39mcriterion(similarity_score, y)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 47\u001b[0m, in \u001b[0;36mEncoderLSTM.forward\u001b[0;34m(self, question1, question2, q1_length, q2_length)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, question1, question2, q1_length, q2_length):\n\u001b[1;32m     46\u001b[0m     encoded_q1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_sentence(question1, q1_length)\n\u001b[0;32m---> 47\u001b[0m     encoded_q2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq2_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     similarity_score\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros(encoded_q1\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(encoded_q1\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]):  \n",
      "Cell \u001b[0;32mIn[20], line 24\u001b[0m, in \u001b[0;36mEncoderLSTM.encode_sentence\u001b[0;34m(self, questions, lengths)\u001b[0m\n\u001b[1;32m     21\u001b[0m embedded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embedded)\n\u001b[1;32m     23\u001b[0m packed_sequence\u001b[38;5;241m=\u001b[39mpack_padded_sequence(embedded, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m out, (hn, cn)\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_sequence\u001b[49m\u001b[43m)\u001b[49m       \n\u001b[1;32m     25\u001b[0m unpacked, _\u001b[38;5;241m=\u001b[39mpad_packed_sequence(out, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, total_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(lengths[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     27\u001b[0m last_token_representation\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros(hn\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/rnn.py:882\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    879\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.embedding=Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm=nn.LSTM(embedding_dim, hidden_size)\n",
    "        self.linear1=nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "        self.linear2=nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.final=nn.Sigmoid()\n",
    "    \n",
    "    def encode_sentence(self, questions, lengths):\n",
    "        sorted_indices=np.flipud(np.argsort(lengths))\n",
    "        lengths=np.flipud(np.sort(lengths))\n",
    "        lengths=lengths.copy()\n",
    "\n",
    "        questions=[torch.LongTensor(questions[i]).to('cpu') for i in sorted_indices]        \n",
    "        questions=pad_sequence(questions, batch_first=True)\n",
    "\n",
    "        embedded=self.embedding(questions).to('cpu')\n",
    "        embedded=self.dropout(embedded)\n",
    "\n",
    "        packed_sequence=pack_padded_sequence(embedded, lengths, batch_first=True)\n",
    "        out, (hn, cn)=self.lstm(packed_sequence)       \n",
    "        unpacked, _=pad_packed_sequence(out, batch_first=True, total_length=int(lengths[0]))\n",
    "\n",
    "        last_token_representation=torch.zeros(hn.size()).permute(1,0,2)\n",
    "\n",
    "        for i in range(hn.size()[1]):\n",
    "            last_token_representation[i]=unpacked[i, lengths[i]-1, :].unsqueeze(0)\n",
    "\n",
    "        out=self.linear2(self.relu(self.linear1(last_token_representation)))\n",
    "\n",
    "        unsorted_output=torch.zeros(out.size())\n",
    "        for i, encoded in enumerate(out):\n",
    "            unsorted_output[sorted_indices[i]]=encoded\n",
    "\n",
    "        return unsorted_output\n",
    "    \n",
    "    def cosine_similarity(self, question1, question2):\n",
    "        return F.cosine_similarity(question1, question2)\n",
    "    def manhattan_distance(self, question1, question2):\n",
    "        return torch.exp(-torch.sum(torch.abs(question1 - question2), dim=0)).to('cpu')\n",
    "\n",
    "    def forward(self, question1, question2, q1_length, q2_length):\n",
    "        encoded_q1=self.encode_sentence(question1, q1_length)\n",
    "        encoded_q2=self.encode_sentence(question2, q2_length)\n",
    "\n",
    "        similarity_score=torch.zeros(encoded_q1.size()[0]).to('cpu')\n",
    "\n",
    "        for i in range(encoded_q1.size()[0]):  \n",
    "            similarity_score[i]=self.manhattan_distance(encoded_q1[i][0], encoded_q2[i][0])\n",
    "            # similarity_score[i] = torch.dot(encoded_q1[i][0], encoded_q2[i][0])\n",
    "        # prediction=self.final(similarity_score)\n",
    "        return similarity_score\n",
    "\n",
    "\n",
    "\n",
    "hidden_size=128\n",
    "embedding_dim=256  \n",
    "loss_history=[]\n",
    "\n",
    "def cosine_loss(cos, target):\n",
    "    loss=0\n",
    "    margin=0\n",
    "    # print(cos, target)\n",
    "    for i in range(len(target)):\n",
    "        if target[i]==1:\n",
    "            loss+=(1-cos[i])\n",
    "        else:\n",
    "            loss+=max(0, cos[i]-margin)\n",
    "    return loss/len(target)\n",
    "\n",
    "\n",
    "model=EncoderLSTM(build_vocab_and_map.n_word, embedding_dim, hidden_size)\n",
    "for i in range(50):\n",
    "    print(i)\n",
    "    criterion=nn.MSELoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters())\n",
    "    losses=[]\n",
    "    for batch in train_dataloader:    \n",
    "        optimizer.zero_grad()\n",
    "        y=torch.Tensor(batch['label']).to('cpu')\n",
    "        # y[y==0]=-1\n",
    "        similarity_score=model(batch['q1'], batch['q2'], batch['q1_length'], batch['q2_length'])\n",
    "        print(similarity_score, y)\n",
    "        loss=criterion(similarity_score, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())    \n",
    "    loss_history.append(sum(losses)/len(losses))\n",
    "    print(loss_history)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(loss_history)), loss_history)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(466.2456, grad_fn=<AddBackward0>),\n",
       " tensor(465.6753, grad_fn=<AddBackward0>),\n",
       " tensor(465.9121, grad_fn=<AddBackward0>),\n",
       " tensor(465.5562, grad_fn=<AddBackward0>),\n",
       " tensor(466.4379, grad_fn=<AddBackward0>),\n",
       " tensor(465.9668, grad_fn=<AddBackward0>),\n",
       " tensor(466.1316, grad_fn=<AddBackward0>),\n",
       " tensor(466.0212, grad_fn=<AddBackward0>),\n",
       " tensor(465.9786, grad_fn=<AddBackward0>),\n",
       " tensor(467.0018, grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.embedding=Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm=nn.LSTM(embedding_dim, hidden_size)\n",
    "        self.linear1=nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "        self.linear2=nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.final=nn.Sigmoid()\n",
    "    \n",
    "    def encode_sentence(self, questions, lengths):\n",
    "        sorted_indices=np.flipud(np.argsort(lengths))\n",
    "        lengths=np.flipud(np.sort(lengths))\n",
    "        lengths=lengths.copy()\n",
    "\n",
    "        questions=[torch.LongTensor(questions[i]).to('cpu') for i in sorted_indices]        \n",
    "        questions=pad_sequence(questions, batch_first=True)\n",
    "\n",
    "        embedded=self.embedding(questions).to('cpu')\n",
    "        embedded=self.dropout(embedded)\n",
    "\n",
    "        packed_sequence=pack_padded_sequence(embedded, lengths, batch_first=True)\n",
    "        out, (hn, cn)=self.lstm(packed_sequence)       \n",
    "        unpacked, _=pad_packed_sequence(out, batch_first=True, total_length=int(lengths[0]))\n",
    "\n",
    "        last_token_representation=torch.zeros(hn.size()).permute(1,0,2)\n",
    "\n",
    "        for i in range(hn.size()[1]):\n",
    "            last_token_representation[i]=unpacked[i, lengths[i]-1, :].unsqueeze(0)\n",
    "\n",
    "        out=self.linear2(self.relu(self.linear1(last_token_representation)))\n",
    "\n",
    "        unsorted_output=torch.zeros(out.size())\n",
    "        for i, encoded in enumerate(out):\n",
    "            unsorted_output[sorted_indices[i]]=encoded\n",
    "\n",
    "        return unsorted_output\n",
    "    \n",
    "    def cosine_similarity(self, question1, question2):\n",
    "        return F.cosine_similarity(question1, question2)\n",
    "    def manhattan_distance(self, question1, question2):\n",
    "        return torch.exp(-torch.sum(torch.abs(question1 - question2), dim=0)).to('cpu')\n",
    "\n",
    "    def forward(self, question1, question2, q1_length, q2_length):\n",
    "        encoded_q1=self.encode_sentence(question1, q1_length)\n",
    "        encoded_q2=self.encode_sentence(question2, q2_length)\n",
    "\n",
    "        similarity_score=torch.zeros(encoded_q1.size()[0]).to('cpu')\n",
    "\n",
    "        for i in range(encoded_q1.size()[0]):  \n",
    "            similarity_score[i]=self.manhattan_distance(encoded_q1[i][0], encoded_q2[i][0])\n",
    "            # similarity_score[i] = torch.dot(encoded_q1[i][0], encoded_q2[i][0])\n",
    "        # prediction=self.final(similarity_score)\n",
    "        return similarity_score\n",
    "\n",
    "\n",
    "\n",
    "hidden_size=128\n",
    "embedding_dim=256  \n",
    "loss_history=[]\n",
    "\n",
    "def cosine_loss(cos, target):\n",
    "    loss=0\n",
    "    margin=0\n",
    "    # print(cos, target)\n",
    "    for i in range(len(target)):\n",
    "        if target[i]==1:\n",
    "            loss+=(1-cos[i])\n",
    "        else:\n",
    "            loss+=max(0, cos[i]-margin)\n",
    "    return loss/len(target)\n",
    "\n",
    "\n",
    "model=EncoderLSTM(build_vocab_and_map.n_word, embedding_dim, hidden_size)\n",
    "for i in range(50):\n",
    "    print(i)\n",
    "    criterion=nn.MSELoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters())\n",
    "    losses=[]\n",
    "    for batch in train_dataloader:    \n",
    "        optimizer.zero_grad()\n",
    "        y=torch.Tensor(batch['label']).to('cpu')\n",
    "        # y[y==0]=-1\n",
    "        similarity_score=model(batch['q1'], batch['q2'], batch['q1_length'], batch['q2_length'])\n",
    "        print(similarity_score, y)\n",
    "        loss=criterion(similarity_score, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())    \n",
    "    loss_history.append(sum(losses)/len(losses))\n",
    "    print(loss_history)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(loss_history)), loss_history)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3765, 0.6911],\n",
       "        [0.8399, 0.8280],\n",
       "        [0.5222, 0.4767]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, 2, requires_grad=True)\n",
    "target = torch.rand(3, 2, requires_grad=False)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4543,  0.0879]) tensor([0.3883, 0.5220])\n"
     ]
    }
   ],
   "source": [
    "m=nn.Sigmoid()\n",
    "input = torch.randn(2)\n",
    "output = m(input)\n",
    "print(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = loss(m(input), target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
