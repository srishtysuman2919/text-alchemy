{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import dropout\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "import torch.nn.functional as F\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3522\n",
      "last session of  day  httptwitpiccomezh\n",
      "[228, 3303, 229, 0]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "train_df=pd.read_csv(\"/Users/srishtysuman/PycharmProjects/NaturalLanguageProcessing/sentiment_analysis_data/train.csv\", encoding='latin1')\n",
    "test_df=pd.read_csv(\"/Users/srishtysuman/PycharmProjects/NaturalLanguageProcessing/sentiment_analysis_data/test.csv\", encoding='latin1')\n",
    "\n",
    "# 1. get only relevant columns as train_df and test_df\n",
    "train_df=train_df[['text', 'sentiment']].dropna()\n",
    "test_df=test_df[['text', 'sentiment']].dropna()\n",
    "\n",
    "# 2. for sentiment, give binary labels\n",
    "def binary_labels(label):\n",
    "    if label==\"positive\":\n",
    "        return 2\n",
    "    elif label==\"neutral\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "train_df[\"sentiment\"]=train_df[\"sentiment\"].apply(binary_labels)\n",
    "test_df[\"sentiment\"]=test_df[\"sentiment\"].apply(binary_labels)\n",
    "test_df\n",
    "\n",
    "def clean_text(text):\n",
    "    if type(text)!=str or pd.isnull(text) or text=='':\n",
    "        return ''\n",
    "    text=text.lower()   \n",
    "    link_re_pattern = \"https?:\\/\\/t.co/[\\w]+\"\n",
    "    text=re.sub(link_re_pattern, '', text)\n",
    "    text=re.sub(\"\\`have\", 'have', text)   \n",
    "    text=re.sub(\"\\`ve\", ' have', text)   \n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\'s\", \" \", text) \n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"can't\", \"can not\", text)\n",
    "    text = re.sub(\"n't\", \" not \", text)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(\\d+)(kK)\", \" \\g<1>000 \", text)\n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r'\\d+', '',text)\n",
    "    text = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', text)\n",
    "    text = text.replace(\"?\",\"\")\n",
    "    text = text.replace(\"(\",\"\")\n",
    "    text = text.replace(\")\",\"\")\n",
    "    text = text.replace('\"',\"\")\n",
    "    text = text.replace(\",\",\"\")\n",
    "    text = text.replace(\"#\",\"\")   \n",
    "    text = text.replace(\"-\",\"\")    \n",
    "    text = text.replace(\"..\",\"\")\n",
    "    text = text.replace(\"/\",\"\")\n",
    "    text = text.replace(\"\\\\\",\"\")\n",
    "    text = text.replace(\":\",\"\")\n",
    "    text = text.replace(\"the\",\"\") \n",
    "    text=re.sub(r'[^\\w\\s]','',text)\n",
    "    text=re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", text)\n",
    "    text=re.sub(\"ii\", \"i\", text)\n",
    "    text=re.sub(\"_\", \"\", text)\n",
    "    text=re.sub(\"^http\", \"\", text) \n",
    "    return text    \n",
    "    \n",
    "train_df[\"text\"]=train_df[\"text\"].apply(clean_text)\n",
    "test_df[\"text\"]=test_df[\"text\"].apply(clean_text)\n",
    "\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "def tokenize(df, test_df):\n",
    "    word_to_index=dict()\n",
    "    index_to_word=['<unk>']\n",
    "    stopword=stopwords.words('english')\n",
    "    count=0\n",
    "    tokenize_column=[]\n",
    "    for index, row in df.iterrows():\n",
    "        text=row[\"text\"]\n",
    "        words=[word for word in text.split()]\n",
    "        token_list=[]\n",
    "        for word in words:\n",
    "            if word in stopword:\n",
    "                continue\n",
    "            if word not in word_to_index:\n",
    "                count+=1\n",
    "                word_to_index[word]=count\n",
    "                index_to_word.append(count)\n",
    "            token_list.append(word_to_index[word])\n",
    "        tokenize_column.append(token_list)\n",
    "    df[\"text_map\"]=tokenize_column\n",
    "\n",
    "    tokenize_column=[]\n",
    "    for index, row in test_df.iterrows():\n",
    "        text=row[\"text\"]\n",
    "        words=[word for word in text.split()]\n",
    "        token_list=[]\n",
    "        for word in words:\n",
    "            if word in stopword:\n",
    "                continue\n",
    "            if word not in word_to_index:\n",
    "                token_list.append(0)\n",
    "            else:\n",
    "                token_list.append(word_to_index[word])\n",
    "        tokenize_column.append(token_list)\n",
    "    test_df[\"text_map\"]=tokenize_column    \n",
    "    return df, test_df, word_to_index, index_to_word\n",
    "\n",
    "train_df, test_df, word_to_index, index_to_word = tokenize(train_df, test_df)\n",
    "\n",
    "test_df = test_df[test_df['text_map'].apply(lambda x: len(x) > 0)].reset_index(drop=True)\n",
    "train_df = train_df[train_df['text_map'].apply(lambda x: len(x) > 0)].reset_index(drop=True)\n",
    "\n",
    "class TextDataloader(Dataset):\n",
    "    def __init__(self, text_list, text_map, labels):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        -------\n",
    "        test : list\n",
    "               list with tuples of all the texts\n",
    "        \n",
    "        word2index : dict\n",
    "                     vocbulary of the dataset\n",
    "        labels : list \n",
    "                 list of the corrsponding labels to the question pairs \n",
    "        \n",
    "        \"\"\"\n",
    "        self.text_list = text_list\n",
    "        self.text_map = text_map\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text_map = self.text_map[index]\n",
    "        text = self.text_list[index]\n",
    "        text=\"\".join(word for word in text)\n",
    "            \n",
    "        # q1_indices and q2_indices are lists of indices against words used in the sentence \n",
    "        return {\n",
    "            'text': text,\n",
    "            'text_map': text_map, \n",
    "            'labels': self.labels[index], \n",
    "        }\n",
    "    \n",
    "def data_to_tuple(df):\n",
    "    text=df[\"text\"].tolist()\n",
    "    text_map=df[\"text_map\"].tolist()\n",
    "    labels=df[\"sentiment\"].tolist()\n",
    "    return text, text_map, labels\n",
    "\n",
    "train_text, train_text_map, train_labels=data_to_tuple(train_df)\n",
    "test_text, test_text_map, test_labels=data_to_tuple(test_df)\n",
    "\n",
    "train_dataset=TextDataloader(train_text, train_text_map, train_labels)\n",
    "test_dataset=TextDataloader(test_text, test_text_map, test_labels)\n",
    "\n",
    "print(len(test_dataset))\n",
    "for sample in test_dataset:\n",
    "    print(sample[\"text\"])\n",
    "    print(sample[\"text_map\"])\n",
    "    print(sample[\"labels\"])   \n",
    "    break\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text = []\n",
    "    text_map = []\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        text.append(item['text'])\n",
    "        text_map.append(item['text_map'])\n",
    "        labels.append(item['labels'])\n",
    "          \n",
    "        \n",
    "    text_lengths = [len(q) for q in text_map]\n",
    "\n",
    "    sorted_indices=np.flipud(np.argsort(text_lengths))\n",
    "    lengths=np.flipud(np.sort(text_lengths))\n",
    "    lengths = lengths.copy()\n",
    "\n",
    "    sorted_text = [text[i] for i in sorted_indices]\n",
    "    sorted_texts_map = [torch.LongTensor(text_map[i]).to('cpu') for i in sorted_indices]\n",
    "    sorted_labels = [labels[i] for i in sorted_indices]\n",
    "\n",
    "    sorted_texts_map = pad_sequence(sorted_texts_map, batch_first=True)    \n",
    "\n",
    "    return {\n",
    "        'text': sorted_text,\n",
    "        'text_map': sorted_texts_map, \n",
    "        'text_lengths': lengths,\n",
    "        'labels': sorted_labels\n",
    "    }\n",
    "\n",
    "train_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=512, collate_fn=collate_fn)\n",
    "test_dataloader=torch.utils.data.DataLoader(test_dataset, batch_size=512, collate_fn=collate_fn)\n",
    "\n",
    "embedding_dim=300\n",
    "embeddings=torch.randn(len(index_to_word), 300)\n",
    "embeddings[0] = torch.zeros(embedding_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of EmbeddingLSTMNet(\n",
      "  (embedding): Embedding(30071, 300)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (encoder_lstm): LSTM(300, 100, batch_first=True)\n",
      "  (decoder_lstm): LSTM(300, 100, batch_first=True)\n",
      "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (batch_norm1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=100, out_features=3, bias=True)\n",
      "  (final_softmax): Softmax(dim=None)\n",
      "  (final_layer): Sigmoid()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention).__init__()\n",
    "    def forward(self, lstm_output, final_state):\n",
    "        weights=torch.bmn(lstm_output, final_state.squeeze(0).unsqueeze(2))\n",
    "        weights=F.softmax(weights.squeeze(2), dim=1).unsqueeze(2)\n",
    "        context_vector=torch.bmn(torch.transpose(weights, 1,2), lstm_output)\n",
    "\n",
    "class EmbeddingLSTMNet(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_cells, num_layers, embedding_grad, embeddings, dropout, num_classes):\n",
    "        super(EmbeddingLSTMNet, self).__init__()\n",
    "        self.device='cpu'\n",
    "        self.embedding=nn.Embedding.from_pretrained(embeddings)\n",
    "        self.embedding.weight.requires_grad=embedding_grad\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder_lstm=nn.LSTM(input_size=embedding_dim, hidden_size=hidden_cells, num_layers=num_layers, batch_first=True)\n",
    "        self.decoder_lstm=nn.LSTM(input_size=embedding_dim, hidden_size=hidden_cells, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.fc1=nn.Linear(hidden_cells, hidden_cells)\n",
    "        self.relu1=nn.ReLU()      \n",
    "        self.batch_norm1=nn.BatchNorm1d(hidden_cells)\n",
    "\n",
    "        self.fc2=nn.Linear(hidden_cells, num_classes)        \n",
    "        self.final_softmax=nn.Softmax()\n",
    "        self.final_layer=nn.Sigmoid()\n",
    "\n",
    "        self.hidden_cells=hidden_cells\n",
    "        # self.attention=Attention()\n",
    "\n",
    "    def attention(self, lstm_output, final_state):\n",
    "        merged_state = final_state.squeeze(0).unsqueeze(2)\n",
    "        weights = torch.bmm(lstm_output, merged_state)\n",
    "        weights = F.softmax(weights.squeeze(2), dim=1).unsqueeze(2)\n",
    "        context_vector=torch.bmm(torch.transpose(lstm_output, 1, 2), weights).squeeze(2)\n",
    "        return context_vector, weights\n",
    "    \n",
    "    def forward(self, texts, texts_map, lengths):\n",
    "        embeddings=self.embedding(texts_map).to(self.device)\n",
    "        embeddings=self.dropout(embeddings)\n",
    "        out, (hn, cn) = self.encoder_lstm(embeddings)\n",
    "\n",
    "        context_vector,attn_weights = self.attention(out, hn)\n",
    "        context_vector=context_vector.unsqueeze(0)\n",
    "        out = self.fc2(self.relu1(self.fc1(torch.cat([context_vector, hn]))))\n",
    "        return out[0]\n",
    "\n",
    "model = EmbeddingLSTMNet(embedding_dim=300, hidden_cells=100, num_layers=1, embedding_grad=True, embeddings=embeddings, dropout=0.0, num_classes=3)\n",
    "print(model.parameters)\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    # print(\"epoch, i\", epoch, i)\n",
    "    text_map, text_lengths = batch['text_map'], batch['text_lengths']\n",
    "    y = torch.tensor(batch['labels'])\n",
    "    y_pred = model(batch[\"text\"], text_map, text_lengths)\n",
    "    loss=nn.CrossEntropyLoss()(y_pred, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 6])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state=torch.randn((3, 6, 1))\n",
    "print(final_state.shape)\n",
    "torch.transpose(final_state, 1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6, 1])\n",
      "torch.Size([3, 6])\n",
      "tensor([[ 1.4574, -0.5846,  0.0139,  0.8708, -2.2888, -0.2166],\n",
      "        [-1.7905,  0.0857, -1.0503, -0.6506,  1.1368,  0.9351],\n",
      "        [-1.2959, -0.1056,  0.3226, -0.2864, -0.5587,  2.0600]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4688],\n",
       "         [0.0608],\n",
       "         [0.1107],\n",
       "         [0.2608],\n",
       "         [0.0111],\n",
       "         [0.0879]],\n",
       "\n",
       "        [[0.0214],\n",
       "         [0.1398],\n",
       "         [0.0449],\n",
       "         [0.0670],\n",
       "         [0.4000],\n",
       "         [0.3269]],\n",
       "\n",
       "        [[0.0233],\n",
       "         [0.0768],\n",
       "         [0.1178],\n",
       "         [0.0641],\n",
       "         [0.0488],\n",
       "         [0.6693]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state=torch.randn((3, 6, 1))\n",
    "print(final_state.shape)\n",
    "print(final_state.squeeze(2).shape)\n",
    "print(final_state.squeeze(2))\n",
    "F.softmax(final_state.squeeze(2), dim=1).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1978,  0.3541,  0.2221, -1.1079, -1.6475, -0.1731],\n",
       "         [ 0.4080,  0.8847,  1.3301, -1.0797,  0.3881, -2.5418],\n",
       "         [ 1.1926,  0.2362, -0.4385, -0.5055, -0.9163,  0.6483],\n",
       "         [ 0.3059, -0.5429,  0.1005,  0.9566,  1.0771, -0.1521]],\n",
       "\n",
       "        [[ 0.4881,  2.5263, -0.9686, -0.2728,  1.8135, -0.0840],\n",
       "         [-0.0694,  0.7219,  1.5815, -0.8289,  0.1787, -1.2577],\n",
       "         [ 1.0936,  1.0060, -1.1942, -0.6852,  1.4193,  0.5340],\n",
       "         [-0.2234, -0.4967, -2.1705,  0.0418, -1.1296, -2.2058]],\n",
       "\n",
       "        [[ 1.2080,  0.6784, -0.5838,  0.0243,  0.9939, -0.5591],\n",
       "         [-0.3657, -1.9982, -0.4795,  1.4495, -1.2987,  0.2616],\n",
       "         [-1.1428,  0.3866, -0.9400,  1.4429, -0.7112, -0.5274],\n",
       "         [-0.2490, -0.1509, -0.6339,  0.8252, -1.4773,  0.1485]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state=torch.randn((3, 4, 6))\n",
    "final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.1978,  0.3541,  0.2221, -1.1079, -1.6475, -0.1731],\n",
       "         [ 0.4080,  0.8847,  1.3301, -1.0797,  0.3881, -2.5418],\n",
       "         [ 1.1926,  0.2362, -0.4385, -0.5055, -0.9163,  0.6483],\n",
       "         [ 0.3059, -0.5429,  0.1005,  0.9566,  1.0771, -0.1521]]),\n",
       " tensor([[ 0.4881,  2.5263, -0.9686, -0.2728,  1.8135, -0.0840],\n",
       "         [-0.0694,  0.7219,  1.5815, -0.8289,  0.1787, -1.2577],\n",
       "         [ 1.0936,  1.0060, -1.1942, -0.6852,  1.4193,  0.5340],\n",
       "         [-0.2234, -0.4967, -2.1705,  0.0418, -1.1296, -2.2058]]),\n",
       " tensor([[ 1.2080,  0.6784, -0.5838,  0.0243,  0.9939, -0.5591],\n",
       "         [-0.3657, -1.9982, -0.4795,  1.4495, -1.2987,  0.2616],\n",
       "         [-1.1428,  0.3866, -0.9400,  1.4429, -0.7112, -0.5274],\n",
       "         [-0.2490, -0.1509, -0.6339,  0.8252, -1.4773,  0.1485]])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in final_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 6])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 18])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_state = torch.cat([s for s in final_state],1)\n",
    "merged_state.shape\n",
    "print(\"after cat\", merged_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 18, 1])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_state.squeeze(0).unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1435,  1.1681, -1.0501,  0.5218, -0.5127, -0.1476],\n",
       "         [ 0.4469,  0.7750,  0.2806,  1.3379, -1.5390,  0.5023],\n",
       "         [-1.1041, -1.3331, -0.2965, -0.8033,  1.3141,  0.3061],\n",
       "         [-0.7461, -0.9973,  0.1945, -1.9226,  0.9130, -1.4198]],\n",
       "\n",
       "        [[-0.4426,  1.0650,  0.3200, -1.0377,  0.5185,  0.8102],\n",
       "         [ 0.0462,  0.7288,  0.3910,  0.9335, -0.1256, -1.6559],\n",
       "         [ 0.7464,  0.8905, -0.3261, -0.7684,  0.7065, -0.4252],\n",
       "         [ 0.6556, -0.5605,  0.3575,  2.0854, -1.0509,  1.3937]],\n",
       "\n",
       "        [[ 0.0108, -0.8162,  1.8186, -2.1920, -0.0894,  0.7893],\n",
       "         [-0.0477,  2.0034, -0.2787, -0.7739,  0.6469, -1.2293],\n",
       "         [-0.0465, -0.2566,  0.6707,  0.9170, -1.4451,  0.0025],\n",
       "         [-1.7753,  0.2129,  2.3884,  0.9750, -0.2691,  0.2106]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "merged_state = merged_state.squeeze(0).unsqueeze(2)\n",
    "print(\"squeeze unsqueeze\", merged_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history=[]\n",
    "for epoch in range(15):\n",
    "    print(\"epoch\", epoch)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    losses=[]\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        # print(\"epoch, i\", epoch, i)\n",
    "        text_map, text_lengths = batch['text_map'], batch['text_lengths']\n",
    "        y = torch.tensor(batch['labels'])\n",
    "        y_pred = model(batch[\"text\"], text_map, text_lengths)\n",
    "        loss=nn.CrossEntropyLoss()(y_pred, y)\n",
    "        # print(y_pred.shape, y.shape)\n",
    "        # print(y_pred)\n",
    "        y_pred_class=[]\n",
    "        for list in y_pred:\n",
    "            y_pred_class.append(torch.argmax(list).detach().numpy().item())   \n",
    "        correct = (torch.tensor(y_pred_class) == torch.tensor(y)).sum().item()\n",
    "        \n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    loss_history.append(sum(losses)/len(losses))\n",
    "print(loss_history)\n",
    "    # print(y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
